{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYjn907suiuF"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Rate Limit Handling\n",
        "rate_limit_time = 1  # Example rate limit time in seconds\n",
        "last_api_call_time = 0\n",
        "\n",
        "def handle_rate_limit():\n",
        "    global last_api_call_time\n",
        "    current_time = time.time()\n",
        "    time_since_last_call = current_time - last_api_call_time\n",
        "    if time_since_last_call < rate_limit_time:\n",
        "        time_to_wait = rate_limit_time - time_since_last_call\n",
        "        print(f\"Rate limit enforced. Waiting for {time_to_wait:.2f} seconds.\")\n",
        "        time.sleep(time_to_wait)\n",
        "    last_api_call_time = current_time\n",
        "\n",
        "# Token Limit Handling\n",
        "max_tokens = 1000 # Example maximum token limit\n",
        "tokens_used = 0\n",
        "\n",
        "def truncate_to_token_limit(text, max_tokens):\n",
        "    tokens = text.split()\n",
        "    if len(tokens) > max_tokens:\n",
        "        truncated_tokens = tokens[:max_tokens]\n",
        "        truncated_text = \" \".join(truncated_tokens)\n",
        "        return truncated_text\n",
        "    return text\n",
        "\n",
        "# Conversation History\n",
        "conversation_history = []\n",
        "\n",
        "def add_to_conversation_history(text):\n",
        "    current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
        "    conversation_history.append(f\"[{current_time}] {text}\")\n",
        "\n",
        "def update_tokens_used(tokens):\n",
        "    global tokens_used\n",
        "    tokens_used += tokens\n",
        "\n",
        "def format_prompt():\n",
        "    conversation = \"\\n\".join(conversation_history)\n",
        "    truncated_conversation = truncate_to_token_limit(conversation, max_tokens - tokens_used)\n",
        "    return f\"{truncated_conversation}\\n{input_text}\"\n",
        "\n",
        "\n",
        "# Custom LLM-like Response Generator\n",
        "def generate_custom_response(prompt):\n",
        "    responses = {\n",
        "        \"Hello\": \"Hello! How can I assist you?\",\n",
        "        \"How are you?\": \"I'm just a computer program, but I'm here to help!\",\n",
        "        \"Tell me a joke\": \"Sure, here's one: Why don't scientists trust atoms? Because they make up everything!\",\n",
        "        \"What's the weather today?\": \"I'm not connected to the internet, so I can't provide real-time weather information.\",\n",
        "        \"Goodbye\": \"Goodbye! Have a great day!\"\n",
        "    }\n",
        "    return responses.get(prompt, \"I'm not sure how to respond to that.\")\n",
        "\n",
        "# Function to call LLM and handle errors\n",
        "def call_llm(prompt):\n",
        "    try:\n",
        "        handle_rate_limit()  # Handle rate limit\n",
        "        llm_response = generate_custom_response(prompt)\n",
        "        tokens_required = len(llm_response.split())\n",
        "        if tokens_used + tokens_required > max_tokens:\n",
        "            raise TokenLimitError(\"Token limit exceeded.\")\n",
        "        update_tokens_used(tokens_required)  # Update tokens used based on LLM response\n",
        "        return llm_response\n",
        "    except RateLimitError as e:\n",
        "        print(\"Rate limit error. Please wait a moment and try again.\")\n",
        "    except TokenLimitError as e:\n",
        "        print(\"Token limit error. Please try again with a shorter input.\")\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred. Please try again later.\")\n",
        "    return \"I'm not sure how to respond to that.\"\n",
        "\n",
        "# Simulating user inputs\n",
        "user_inputs = [\n",
        "    \"Hello\",\n",
        "    \"How are you?\",\n",
        "    \"Tell me a joke\",\n",
        "    \"What's the weather today?\",\n",
        "    \"Goodbye\"\n",
        "]\n",
        "\n",
        "# Main loop for simulating conversation\n",
        "for input_text in user_inputs:\n",
        "    prompt = format_prompt()\n",
        "    llm_response = call_llm(input_text)\n",
        "    add_to_conversation_history(f\"User Input: {input_text}\")\n",
        "    add_to_conversation_history(f\"LLM Response: {llm_response}\")  # Add LLM response to history\n",
        "\n",
        "    # Print conversation history and remaining tokens\n",
        "    print(\"\\n\".join(conversation_history))\n",
        "    print(\"Remaining Tokens:\", max_tokens - tokens_used)\n",
        "    print(\"=\" * 30)\n"
      ]
    }
  ]
}