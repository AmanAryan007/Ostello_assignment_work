{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#This is Custom response Based\n",
        "You Can also Apply different model and docs based answer they give automatically\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "bkC30cHu3mJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Rate Limit Handling\n",
        "#\"\"\"Rate limiting is used to prevent excessive requests being made to a service or API in a short period of time. It's a common practice to ensure that the service remains available, prevent abuse, and avoid overloading the system.\"\"\"\n",
        "rate_limit_time = 1  # Example rate limit time in seconds\n",
        "last_api_call_time = 0\n",
        "\n",
        "def handle_rate_limit():\n",
        "    global last_api_call_time\n",
        "    current_time = time.time()\n",
        "    time_since_last_call = current_time - last_api_call_time\n",
        "    if time_since_last_call < rate_limit_time:\n",
        "        time_to_wait = rate_limit_time - time_since_last_call\n",
        "        print(f\"Rate limit enforced. Waiting for {time_to_wait:.2f} seconds.\")\n",
        "        time.sleep(time_to_wait)\n",
        "    last_api_call_time = current_time\n",
        "\n",
        "# Token Limit Handling\n",
        "max_tokens = 1000  # Example maximum token limit\n",
        "tokens_used = 0\n",
        "\n",
        "def truncate_to_token_limit(text, max_tokens):\n",
        "    tokens = text.split()\n",
        "    if len(tokens) > max_tokens:\n",
        "        truncated_tokens = tokens[:max_tokens]\n",
        "        truncated_text = \" \".join(truncated_tokens)\n",
        "        return truncated_text\n",
        "    return text\n",
        "\n",
        "# Conversation History\n",
        "conversation_history = []\n",
        "\n",
        "def add_to_conversation_history(text):\n",
        "    conversation_history.append(text)\n",
        "\n",
        "def update_tokens_used(tokens):\n",
        "    global tokens_used\n",
        "    tokens_used += tokens\n",
        "\n",
        "# Formatting Prompts\n",
        "def format_prompt():\n",
        "    truncated_conversation = truncate_to_token_limit(\" \".join(conversation_history), max_tokens - tokens_used)\n",
        "    return \"\\n\".join(conversation_history)\n",
        "\n",
        "# Custom LLM-like Response Generator\n",
        "def generate_custom_response(prompt):\n",
        "    responses = {\n",
        "        \"Hello\": \"Hello! How can I assist you?\",\n",
        "        \"How are you?\": \"I'm just a computer program, but I'm here to help!\",\n",
        "        \"Tell me a joke\": \"Sure, here's one: Why don't scientists trust atoms? Because they make up everything!\",\n",
        "        \"What's the weather today?\": \"I'm not connected to the internet, so I can't provide real-time weather information.\",\n",
        "        \"Goodbye\": \"Goodbye! Have a great day!\"\n",
        "    }\n",
        "    return responses.get(prompt, \"I'm not sure how to respond to that.\")\n",
        "\n",
        "# Simulating user inputs\n",
        "user_inputs = [\n",
        "    \"Hello\",\n",
        "    \"How are you?\",\n",
        "    \"Tell me a joke\",\n",
        "    \"What's the weather today?\",\n",
        "    \"Goodbye\"\n",
        "]\n",
        "\n",
        "# Main loop for simulating conversation\n",
        "for input_text in user_inputs:\n",
        "    prompt = format_prompt()\n",
        "    llm_response = generate_custom_response(input_text)\n",
        "    update_tokens_used(len(llm_response.split()))  # Update tokens used based on LLM response\n",
        "    remaining_tokens = max_tokens - tokens_used\n",
        "    add_to_conversation_history(f\"User Input: {input_text}\")\n",
        "    add_to_conversation_history(f\"LLM Response: {llm_response}\")  # Add LLM response to history\n",
        "\n",
        "    # Print conversation history and remaining tokens\n",
        "    print(\"\\n\".join(conversation_history))\n",
        "    print(\"Remaining Tokens:\", remaining_tokens)\n",
        "    print(\"=\" * 30)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYsDmm8fAg9u",
        "outputId": "34fbc236-a344-4716-e761-702256d741b6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User Input: Hello\n",
            "LLM Response: Hello! How can I assist you?\n",
            "Remaining Tokens: 994\n",
            "==============================\n",
            "User Input: Hello\n",
            "LLM Response: Hello! How can I assist you?\n",
            "User Input: How are you?\n",
            "LLM Response: I'm just a computer program, but I'm here to help!\n",
            "Remaining Tokens: 984\n",
            "==============================\n",
            "User Input: Hello\n",
            "LLM Response: Hello! How can I assist you?\n",
            "User Input: How are you?\n",
            "LLM Response: I'm just a computer program, but I'm here to help!\n",
            "User Input: Tell me a joke\n",
            "LLM Response: Sure, here's one: Why don't scientists trust atoms? Because they make up everything!\n",
            "Remaining Tokens: 971\n",
            "==============================\n",
            "User Input: Hello\n",
            "LLM Response: Hello! How can I assist you?\n",
            "User Input: How are you?\n",
            "LLM Response: I'm just a computer program, but I'm here to help!\n",
            "User Input: Tell me a joke\n",
            "LLM Response: Sure, here's one: Why don't scientists trust atoms? Because they make up everything!\n",
            "User Input: What's the weather today?\n",
            "LLM Response: I'm not connected to the internet, so I can't provide real-time weather information.\n",
            "Remaining Tokens: 958\n",
            "==============================\n",
            "User Input: Hello\n",
            "LLM Response: Hello! How can I assist you?\n",
            "User Input: How are you?\n",
            "LLM Response: I'm just a computer program, but I'm here to help!\n",
            "User Input: Tell me a joke\n",
            "LLM Response: Sure, here's one: Why don't scientists trust atoms? Because they make up everything!\n",
            "User Input: What's the weather today?\n",
            "LLM Response: I'm not connected to the internet, so I can't provide real-time weather information.\n",
            "User Input: Goodbye\n",
            "LLM Response: Goodbye! Have a great day!\n",
            "Remaining Tokens: 953\n",
            "==============================\n"
          ]
        }
      ]
    }
  ]
}